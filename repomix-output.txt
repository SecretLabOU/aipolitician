This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.repomix/
  bundles.json
db/
  milvus/
    scripts/
      initialize_db.py
      schema.py
      search.py
    test/
      insert_sample.py
      test_search.py
    cleanup.sh
    docker-compose.yml
    README.md
    setup.sh
test/
  test_biden_model.py
  test_db.py
  test_trump_model.py
training/
  train_mistral_biden.py
  train_mistral_trump.py
.env.example
.gitignore
chat_biden.py
chat_trump.py
README.md
requirements-chat.txt
requirements-training.txt

================================================================
Files
================================================================

================
File: .repomix/bundles.json
================
{
  "bundles": {}
}

================
File: db/milvus/scripts/initialize_db.py
================
#!/usr/bin/env python3
import sys
import os
import logging
import traceback
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

#!/usr/bin/env python3
"""
Initialize the Milvus database for AI Politician project.
This script sets up the Milvus collection and indexes.
"""

import os
import sys
from pathlib import Path
current_dir = Path(__file__).resolve().parent
parent_dir = current_dir.parent.parent
sys.path.append(str(parent_dir))

from scripts.schema import connect_to_milvus, create_political_figures_collection, create_hnsw_index

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(current_dir.parent, "logs", "initialize_db.log"))
    ]
)

logger = logging.getLogger(__name__)

def wait_for_milvus_ready(max_attempts=30, wait_time=5):
    """Wait for Milvus server to be ready, with retry"""
    logger.info("Waiting for Milvus server to be ready...")
    
    for attempt in range(max_attempts):
        try:
            if connect_to_milvus():
                logger.info("Milvus server is ready")
                return True
            else:
                logger.warning(f"Milvus not ready yet. Attempt {attempt+1}/{max_attempts}")
                time.sleep(wait_time)
        except Exception as e:
            logger.warning(f"Error connecting to Milvus: {str(e)}. Attempt {attempt+1}/{max_attempts}")
            time.sleep(wait_time)
    
    logger.error(f"Milvus server not ready after {max_attempts} attempts")
    return False

def initialize_database(recreate=False):
    """Initialize the database with collections and indexes"""
    logger.info("Initializing Milvus database...")
    
    # Wait for Milvus to be ready
    if not wait_for_milvus_ready():
        logger.error("Failed to connect to Milvus. Please check if Milvus is running.")
        return False
    
    try:
        # Create collection
        collection = create_political_figures_collection(drop_existing=recreate)
        logger.info(f"Collection ready: {collection.name}")
        
        # Create index if needed
        if recreate or not collection.has_index():
            create_hnsw_index(collection.name)
            logger.info("Index created successfully")
        else:
            logger.info("Index already exists")
        
        logger.info("Database initialization completed successfully")
        return True
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Initialize Milvus database for AI Politician")
    parser.add_argument("--recreate", action="store_true", help="Drop and recreate existing collections")
    args = parser.parse_args()
    
    success = initialize_database(recreate=args.recreate)
    
    if success:
        print("Database initialization completed successfully")
        sys.exit(0)
    else:
        print("Database initialization failed. See logs for details.")
        sys.exit(1)

================
File: db/milvus/scripts/schema.py
================
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

"""
AI Politician Milvus Database Schema

This module defines the schema for the Milvus database used to store and retrieve
political figure information for the AI debate system.
"""

from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def connect_to_milvus(host="localhost", port="19530"):
    """
    Establish connection to Milvus server.
    
    Args:
        host (str): Milvus server hostname
        port (str): Milvus server port
        
    Returns:
        bool: True if connection was successful
    """
    try:
        connections.connect("default", host=host, port=port)
        logger.info(f"Connected to Milvus server at {host}:{port}")
        return True
    except Exception as e:
        logger.error(f"Failed to connect to Milvus: {str(e)}")
        return False

def create_political_figures_collection(drop_existing=False):
    """
    Create the political_figures collection with appropriate schema.
    
    Args:
        drop_existing (bool): Whether to drop the collection if it exists
        
    Returns:
        Collection: The created or existing collection
    """
    collection_name = "political_figures"
    
    # Check if collection exists
    if utility.has_collection(collection_name):
        if drop_existing:
            logger.info(f"Dropping existing collection '{collection_name}'")
            utility.drop_collection(collection_name)
        else:
            logger.info(f"Collection '{collection_name}' already exists")
            return Collection(name=collection_name)
    
    # Define fields
    fields = [
        FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=36, is_primary=True),
        FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=255),
        FieldSchema(name="date_of_birth", dtype=DataType.VARCHAR, max_length=10),  # Store as YYYY-MM-DD
        FieldSchema(name="nationality", dtype=DataType.VARCHAR, max_length=100),
        FieldSchema(name="political_affiliation", dtype=DataType.VARCHAR, max_length=100),
        FieldSchema(name="biography", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="positions", dtype=DataType.JSON),
        FieldSchema(name="policies", dtype=DataType.JSON),
        FieldSchema(name="legislative_actions", dtype=DataType.JSON),
        FieldSchema(name="public_communications", dtype=DataType.JSON),
        FieldSchema(name="timeline", dtype=DataType.JSON),
        FieldSchema(name="campaigns", dtype=DataType.JSON),
        FieldSchema(name="media", dtype=DataType.JSON),
        FieldSchema(name="philanthropy", dtype=DataType.JSON),
        FieldSchema(name="personal_details", dtype=DataType.JSON),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
    ]
    
    schema = CollectionSchema(fields, description="Political figures vector database for AI debate system")
    collection = Collection(name=collection_name, schema=schema)
    
    logger.info(f"Collection '{collection_name}' created successfully")
    return collection

def create_hnsw_index(collection_name="political_figures"):
    """
    Create HNSW index on the embedding field.
    
    Args:
        collection_name (str): Name of the collection to index
        
    Returns:
        Collection: The indexed collection
    """
    try:
        collection = Collection(name=collection_name)
        
        # Define index parameters
        index_params = {
            "index_type": "HNSW",
            "metric_type": "L2",
            "params": {
                "M": 16,                # Max number of connections per layer
                "efConstruction": 200,  # Size of dynamic candidate list
            }
        }
        
        # Create index on the embedding field
        collection.create_index(field_name="embedding", index_params=index_params)
        logger.info(f"HNSW index created on 'embedding' field in '{collection_name}' collection")
        
        # Configure search parameters
        collection.load()
        logger.info(f"Collection '{collection_name}' loaded into memory")
        
        return collection
    except Exception as e:
        logger.error(f"Failed to create index: {str(e)}")
        raise

def initialize_database():
    """
    Initialize the database by connecting, creating collection, and index.
    
    Returns:
        Collection: The initialized collection
    """
    if not connect_to_milvus():
        raise ConnectionError("Could not connect to Milvus database")
    
    collection = create_political_figures_collection()
    create_hnsw_index(collection.name)
    
    return collection

if __name__ == "__main__":
    print("Initializing Milvus database schema...")
    initialize_database()
    print("Schema initialization complete!")

================
File: db/milvus/scripts/search.py
================
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
"""
AI Politician Milvus Search Utilities

This module provides functions for searching and retrieving political figure
information from the Milvus vector database.
"""

from pymilvus import Collection, connections, utility
from sentence_transformers import SentenceTransformer
import logging
import json
import uuid

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize the sentence transformer model for generating embeddings
model = None

def get_embedding_model():
    """
    Get or initialize the sentence transformer model.
    
    Returns:
        SentenceTransformer: The embedding model
    """
    global model
    if model is None:
        try:
            # Using a well-supported model for embedding generation
            model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("Embedding model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {str(e)}")
            raise
    return model

def connect_to_milvus(host="localhost", port="19530"):
    """
    Establish connection to Milvus server.
    
    Args:
        host (str): Milvus server hostname
        port (str): Milvus server port
        
    Returns:
        bool: True if connection was successful
    """
    try:
        connections.connect("default", host=host, port=port)
        logger.info(f"Connected to Milvus server at {host}:{port}")
        return True
    except Exception as e:
        logger.error(f"Failed to connect to Milvus: {str(e)}")
        return False

def get_collection(collection_name="political_figures"):
    """
    Get a Milvus collection and ensure it's loaded.
    
    Args:
        collection_name (str): Name of the collection
        
    Returns:
        Collection: The Milvus collection
    """
    if not utility.has_collection(collection_name):
        raise ValueError(f"Collection '{collection_name}' does not exist")
    
    collection = Collection(name=collection_name)
    
    # Load collection if not already loaded
    if not collection.is_loaded():
        collection.load()
        logger.info(f"Collection '{collection_name}' loaded into memory")
    
    return collection

def search_political_figures(query, limit=5, output_fields=None):
    """
    Search for political figures based on semantic similarity to the query.
    
    Args:
        query (str): The search query
        limit (int): Maximum number of results to return
        output_fields (list): List of fields to include in the results
        
    Returns:
        list: List of search results
    """
    # Connect to Milvus
    if not connections.has_connection("default"):
        connect_to_milvus()
    
    # Default output fields if none provided
    if output_fields is None:
        output_fields = ["name", "political_affiliation", "biography"]
    
    # Get collection
    collection = get_collection()
    
    # Generate embedding for query
    model = get_embedding_model()
    query_embedding = model.encode(query).tolist()
    
    # Search parameters
    search_params = {
        "metric_type": "L2",
        "params": {"ef": 100}  # Size of final candidate list for search
    }
    
    # Execute search
    try:
        results = collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=limit,
            output_fields=output_fields
        )
        
        # Format results
        search_results = []
        for hits in results:
            for hit in hits:
                result = {field: hit.entity.get(field) for field in output_fields}
                result["score"] = 1.0 / (1.0 + hit.distance)  # Convert distance to similarity score
                search_results.append(result)
        
        return search_results
    except Exception as e:
        logger.error(f"Search failed: {str(e)}")
        raise

def insert_political_figure(politician_data):
    """
    Insert a political figure into the database.
    
    Args:
        politician_data (dict): Dictionary containing politician information
        
    Returns:
        str: The ID of the inserted politician
    """
    # Connect to Milvus
    if not connections.has_connection("default"):
        connect_to_milvus()
    
    # Get collection
    collection = get_collection()
    
    # Ensure politician_data has all required fields
    required_fields = ["name", "biography"]
    for field in required_fields:
        if field not in politician_data:
            raise ValueError(f"Missing required field: {field}")
    
    # Generate embedding for biography
    model = get_embedding_model()
    embedding = model.encode(politician_data["biography"]).tolist()
    
    # Generate UUID if not provided
    if "id" not in politician_data:
        politician_data["id"] = str(uuid.uuid4())
    
    # Ad

================
File: db/milvus/test/insert_sample.py
================
#!/usr/bin/env python3
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

#!/usr/bin/env python3
"""
Insert sample data into the Milvus database.
This script adds a test political figure to verify database functionality.
"""

import sys
import json
import uuid
from pathlib import Path
import logging
from sentence_transformers import SentenceTransformer

# Add the parent directory to the Python path
current_dir = Path(__file__).resolve().parent
parent_dir = current_dir.parent.parent
sys.path.append(str(parent_dir))

from scripts.schema import connect_to_milvus, create_political_figures_collection

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def insert_sample_data():
    """Insert a sample political figure record"""
    # Connect to Milvus
    if not connect_to_milvus():
        logger.error("Failed to connect to Milvus")
        return False
    
    # Get the collection
    collection = create_political_figures_collection()
    
    # Load the embedding model that produces 768-dimensional vectors
    model = SentenceTransformer('all-mpnet-base-v2')  # This model produces 768-dimensional vectors
    
    # Sample data for Joe Biden
    sample_data = {
        "id": str(uuid.uuid4()),
        "name": "Joe Biden",
        "date_of_birth": "1942-11-20",
        "nationality": "American",
        "political_affiliation": "Democratic Party",
        "biography": "Joseph Robinette Biden Jr. is an American politician who is the 46th president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.",
        "positions": json.dumps([
            {"title": "President of the United States", "start_year": 2021, "end_year": None},
            {"title": "Vice President of the United States", "start_year": 2009, "end_year": 2017},
            {"title": "U.S. Senator from Delaware", "start_year": 1973, "end_year": 2009}
        ]),
        "policies": json.dumps({
            "healthcare": "Supporter of the Affordable Care Act",
            "climate": "Committed to addressing climate change, rejoined Paris Agreement",
            "economy": "Focus on middle-class growth and infrastructure investment"
        }),
        "legislative_actions": json.dumps([
            {"title": "American Rescue Plan", "year": 2021, "description": "COVID-19 economic stimulus package"},
            {"title": "Infrastructure Investment and Jobs Act", "year": 2021, "description": "Bipartisan infrastructure bill"}
        ]),
        "public_communications": json.dumps({
            "speeches": ["Inaugural Address", "State of the Union"],
            "interviews": ["60 Minutes", "CNN Town Hall"]
        }),
        "timeline": json.dumps([
            {"year": 1942, "event": "Born in Scranton, Pennsylvania"},
            {"year": 1972, "event": "Elected to the U.S. Senate"},
            {"year": 2009, "event": "Became Vice President"},
            {"year": 2021, "event": "Inaugurated as 46th President"}
        ]),
        "campaigns": json.dumps([
            {"year": 1988, "position": "President", "outcome": "Withdrew during primaries"},
            {"year": 2008, "position": "President", "outcome": "Withdrew during primaries"},
            {"year": 2020, "position": "President", "outcome": "Won general election"}
        ]),
        "media": json.dumps({
            "books": ["Promise Me, Dad", "Promises to Keep"],
            "documentaries": ["Joe Biden: American Dreamer"]
        }),
        "philanthropy": json.dumps({
            "foundations": ["Biden Foundation", "Biden Cancer Initiative"],
            "causes": ["Cancer research", "Violence against women"]
        }),
        "personal_details": json.dumps({
            "religion": "Roman Catholic",
            "education": "University of Delaware, Syracuse University Law School",
            "family": "Married to Jill Biden, father to Hunter, Ashley, and the late Beau and Naomi"
        })
    }
    
    # Generate embedding from biography
    sample_data["embedding"] = model.encode(sample_data["biography"]).tolist()
    
    # Insert the data
    try:
        collection.insert([sample_data])
        logger.info(f"Sample record for {sample_data['name']} inserted successfully")
        logger.info(f"Record ID: {sample_data['id']}")
        return True
    except Exception as e:
        logger.error(f"Failed to insert sample record: {str(e)}")
        return False

if __name__ == "__main__":
    print("Inserting sample data into Milvus database...")
    if insert_sample_data():
        print("Sample data inserted successfully!")
    else:
        print("Failed to insert sample data.")
        sys.exit(1)

================
File: db/milvus/test/test_search.py
================
#!/usr/bin/env python3
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

#!/usr/bin/env python3
import sys
import logging
from sentence_transformers import SentenceTransformer
from pymilvus import Collection, connections

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def connect_to_milvus(host="localhost", port="19530"):
    """Establish connection to Milvus server."""
    try:
        connections.connect("default", host=host, port=port)
        logger.info(f"Connected to Milvus server at {host}:{port}")
        return True
    except Exception as e:
        logger.error(f"Failed to connect to Milvus: {str(e)}")
        return False

def search_politicians(query, limit=3):
    """Search for politicians matching the query."""
    # Connect to Milvus
    if not connect_to_milvus():
        logger.error("Failed to connect to Milvus")
        return
    
    # Get the collection
    collection = Collection(name="political_figures")
    collection.load()
    
    # Load the embedding model
    model = SentenceTransformer('all-mpnet-base-v2')  # Using model that produces 768-dim vectors
    
    # Generate embedding for query
    query_embedding = model.encode(query).tolist()
    
    # Define search parameters
    search_params = {
        "metric_type": "L2",
        "params": {"ef": 100}
    }
    
    # Execute search
    results = collection.search(
        data=[query_embedding], 
        anns_field="embedding", 
        param=search_params,
        limit=limit,
        output_fields=["id", "name", "political_affiliation", "biography"]
    )
    
    # Process results
    search_results = []
    for hits in results:
        for hit in hits:
            result = {
                "id": hit.entity.get("id"),
                "name": hit.entity.get("name"),
                "political_affiliation": hit.entity.get("political_affiliation"),
                "biography": hit.entity.get("biography"),
                "similarity": hit.score
            }
            search_results.append(result)
            
    return search_results

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python test_search.py <search_query>")
        sys.exit(1)
        
    query = sys.argv[1]
    print(f"Searching for: {query}")
    
    results = search_politicians(query)
    
    if not results:
        print("No results found.")
    else:
        print(f"\nFound {len(results)} results:")
        for i, result in enumerate(results, 1):
            print(f"\n--- Result {i} ---")
            print(f"Name: {result['name']}")
            print(f"Political Affiliation: {result['political_affiliation']}")
            print(f"Similarity Score: {result['similarity']:.4f}")
            print(f"Biography: {result['biography'][:200]}...")

================
File: db/milvus/cleanup.sh
================
#!/bin/bash

# Cleanup script for Milvus database
# This script stops Docker containers and optionally removes data

# Define colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Print section header
section() {
    echo -e "\n${GREEN}=== $1 ===${NC}\n"
}

# Print information messages
info() {
    echo -e "${YELLOW}INFO:${NC} $1"
}

# Print error messages
error() {
    echo -e "${RED}ERROR:${NC} $1"
}

# Function to ask for confirmation
confirm() {
    read -p "$1 (y/n): " answer
    case ${answer:0:1} in
        y|Y )
            return 0
        ;;
        * )
            return 1
        ;;
    esac
}

# Check if Docker is installed
section "Checking Prerequisites"
if ! command -v docker &> /dev/null; then
    error "Docker is not installed. Cannot proceed with cleanup."
    exit 1
fi

if ! command -v docker compose &> /dev/null; then
    error "Docker Compose is not installed. Cannot proceed with cleanup."
    exit 1
fi

# Stop Milvus containers
section "Stopping Milvus Containers"
info "Stopping all Milvus-related containers using Docker Compose"
cd "$(dirname "$0")"

if docker ps | grep -q "ai_politician_milvus\|milvus-etcd\|milvus-minio"; then
    docker compose down
    info "Milvus containers have been stopped"
else
    info "No Milvus containers are currently running"
fi

# Data cleanup section - now with a clearer warning
section "Data Management"
echo -e "${YELLOW}NOTE:${NC} For normal usage, you do NOT need to delete your data."
echo -e "Only delete data if you're sure you want to reset everything and lose all stored information."

if confirm "Do you want to remove all Milvus data? This is NOT recommended for regular cleanup and will DELETE ALL stored data (cannot be undone)"; then
    if confirm "Are you ABSOLUTELY SURE? This will permanently delete ALL database content"; then
        info "Removing Milvus data directories..."
        rm -rf /home/natalie/Databases/ai_politician_milvus/data/*
        rm -rf /home/natalie/Databases/ai_politician_milvus/etcd/*
        rm -rf /home/natalie/Databases/ai_politician_milvus/minio/*
        info "Milvus data has been removed"
    else
        info "Data deletion cancelled. Your data is preserved."
    fi
else
    info "Good choice! Data cleanup skipped. Your database content is preserved for future use."
fi

# Success message
section "Cleanup Complete"
echo -e "${GREEN}Milvus database cleanup is complete!${NC}"
echo -e "All Milvus containers have been stopped but your data is preserved."
echo -e "To restart Milvus, run the setup.sh script."

================
File: db/milvus/docker-compose.yml
================
services:
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.18
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - /home/natalie/Databases/ai_politician_milvus/etcd:/etcd
    command: etcd -advertise-client-urls=http://etcd:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9000:9000"
    volumes:
      - /home/natalie/Databases/ai_politician_milvus/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  standalone:
    container_name: ai_politician_milvus
    image: milvusdb/milvus:v2.5.5
    command: ["milvus", "run", "standalone"]
    security_opt:
    - seccomp:unconfined
    environment:
      MINIO_REGION: us-east-1
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - /home/natalie/Databases/ai_politician_milvus/data:/var/lib/milvus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"

networks:
  default:
    name: milvus
    external: true

================
File: db/milvus/README.md
================
# Milvus Database for Political Figure Retrieval

## 1. Overview

This database is designed for real-time political figure retrieval to support AI debates. It leverages Milvus as a vector database to enable semantic search functionality for political information.

Key Milvus features used in this project:
- **Vector Similarity Search**: Find relevant political information using semantic similarity
- **Schema Flexibility**: Combine structured data with vector embeddings
- **HNSW Indexing**: High-performance approximate nearest neighbor search

## 2. Installation

### Docker Deployment

The easiest way to set up the environment is using the provided scripts:

```bash
# Set up Milvus and dependencies
./setup.sh

# To tear down the environment when done
./cleanup.sh
```

Alternatively, you can run Milvus directly with Docker:

```bash
docker run -d --name milvus -p 19530:19530 milvusdb/milvus:latest
```

Or use the provided docker-compose.yml file:

```bash
docker compose up -d
```

### Python Dependencies

```python
pymilvus==2.2.0
sentence-transformers==2.2.2
```

You can install the dependencies using:

```bash
pip install pymilvus==2.2.0 sentence-transformers==2.2.2
```

## 3. Database Schema

The political figures collection has the following schema:

| Field | Type | Description |
|---|---|---|
| id | VARCHAR(36) | Primary key, UUID |
| name | VARCHAR(255) | Political figure's name |
| date_of_birth | VARCHAR(10) | Birth date (YYYY-MM-DD) |
| nationality | VARCHAR(100) | Country of origin |
| political_affiliation | VARCHAR(100) | Party or political leaning |
| biography | VARCHAR(65535) | Full biography text |
| positions | JSON | Political positions held |
| policies | JSON | Political stance data |
| legislative_actions | JSON | Voting records and sponsored bills |
| public_communications | JSON | Speeches and public statements |
| timeline | JSON | Key events timeline |
| campaigns | JSON | Campaign information |
| media | JSON | Media appearances and links |
| philanthropy | JSON | Charitable activities |
| personal_details | JSON | Additional personal information |
| embedding | FLOAT_VECTOR(768) | all-MiniLM-L6-v2 output vector |

## 4. Script Usage

### Database Initialization

To initialize or reset the database:

```bash
python3 scripts/initialize_db.py --recreate
```

This script:
- Establishes connection to Milvus
- Creates the political_figures collection with the schema above
- Builds HNSW index on the embedding field

### Search API Usage

Example code for searching political figures:

```python
from scripts.search import search_political_figures

# Search for political figures based on a query
results = search_political_figures(
    query="What is the stance on climate change?",
    limit=5,
    output_fields=["name", "political_affiliation", "policies"]
)

# Print results
for result in results:
    print(f"Name: {result['name']}")
    print(f"Affiliation: {result['political_affiliation']}")
    print(f"Score: {result['score']}")
    print(f"Policies: {result['policies']}")
    print("---")
```

## 5. Embedding Strategy

This project uses the following embedding approach:

- **Model**: all-MiniLM-L6-v2 sentence transformer model
- **Vector Dimensions**: 768-dimensional float vectors
- **Index Type**: HNSW (Hierarchical Navigable Small World)
- **Index Parameters**:
    - M=16 (maximum number of connections per layer)
    - efConstruction=200 (size of dynamic candidate list during construction)
- **Search Parameters**:
    - ef=100 (size of the dynamic candidate list during search)
    - Metric: L2 (Euclidean distance)

The embedding model converts politician biographical text and policy statements into numerical vectors, enabling semantic similarity search.

## 6. Directory Structure

```
./
├── scripts/
│   ├── initialize_db.py   # Database initialization
│   ├── schema.py          # Collection definition
│   └── search.py          # Vector query logic
├── logs/                  # Application logs
├── docker-compose.yml     # Docker configuration
├── setup.sh               # Setup script for environment
├── cleanup.sh             # Cleanup script for environment
└── README.md              # This documentation
```

## 7. Troubleshooting

### Common Errors

**Milvus Connection Issues**
- Error: "Failed to connect to Milvus server"
- Solution: Ensure Milvus container is running with `docker ps` and check logs with `docker logs milvus`

**Index Creation Failures**
- Error: "Failed to create index"
- Solution: Verify sufficient memory is available for index creation
- Alternative: Adjust index parameters (M, efConstruction) for lower memory usage

**JSON Field Format Requirements**
- Error: "Invalid field value type"
- Solution: Ensure JSON fields contain valid JSON structures
- Example: `policies` field should use the format:
  ```json
  {
    "climate_change": {"position": "supportive", "details": "..."},
    "economy": {"position": "mixed", "details": "..."}
  }
  ```

### Performance Optimization

- Increase `M` parameter for better search accuracy (at the cost of memory)
- Decrease `efConstruction` for faster index building (at the cost of recall)
- Adjust `ef` search parameter based on query performance requirements

================
File: db/milvus/setup.sh
================
#!/bin/bash

# Setup script for Milvus database
# This script initializes the Milvus database for the AI Politician project

# Exit on error
set -e

# Define colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Print section header
section() {
    echo -e "\n${GREEN}=== $1 ===${NC}\n"
}

# Print information messages
info() {
    echo -e "${YELLOW}INFO:${NC} $1"
}

# Print error messages
error() {
    echo -e "${RED}ERROR:${NC} $1"
}

# Check if Docker is installed
section "Checking Prerequisites"
if ! command -v docker &> /dev/null; then
    error "Docker is not installed. Please install Docker first."
    exit 1
fi

if ! command -v docker compose &> /dev/null; then
    error "Docker Compose is not installed. Please install Docker Compose first."
    exit 1
fi

# Create database directories
section "Creating Database Directories"
info "Setting up directories at /home/natalie/Databases/ai_politician_milvus"

mkdir -p /home/natalie/Databases/ai_politician_milvus/data
mkdir -p /home/natalie/Databases/ai_politician_milvus/conf
mkdir -p /home/natalie/Databases/ai_politician_milvus/logs

# Set proper permissions
info "Setting directory permissions"
chmod -R 755 /home/natalie/Databases/ai_politician_milvus

# Start Milvus with Docker Compose
section "Starting Milvus Database"
info "Starting Milvus container using Docker Compose"
cd "$(dirname "$0")"
docker compose up -d

# Wait for Milvus to be ready
info "Waiting for Milvus to be ready..."
sleep 10

# Check if Milvus is running
if docker ps | grep -q ai_politician_milvus; then
    info "Milvus container is running"
else
    error "Milvus container is not running. Please check docker logs."
    exit 1
fi

# Success message
section "Setup Complete"
echo -e "${GREEN}Milvus database setup is complete!${NC}"
echo -e "Milvus server is running at: localhost:19530"
echo -e "You can now use the Python scripts to create collections and indexes."
echo -e "\nTo stop Milvus: docker-compose down"
echo -e "To view logs: docker logs ai_politician_milvus"

================
File: test/test_biden_model.py
================
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig

def main():
    print("Loading Biden model from HuggingFace...")
    LORA_PATH = "nnat03/biden-mistral-adapter"
    BASE_MODEL_PATH = "mistralai/Mistral-7B-Instruct-v0.2"
    
    print(f"Using base model: {BASE_MODEL_PATH}")
    print(f"Using LoRA adapter: {LORA_PATH}")
    
    # Load base model
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16"
    )
    
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        print("Base model loaded successfully!")
        
        # Load LoRA model
        model = PeftModel.from_pretrained(base_model, LORA_PATH)
        print("LoRA adapter loaded successfully!")
        
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)
        print("Tokenizer loaded successfully!")
        
        print("All components loaded successfully!")
        return True
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        return False

if __name__ == "__main__":
    main()

================
File: test/test_db.py
================
#!/usr/bin/env python3
"""
Test script for the political RAG database system.
"""
import sys
from db.database import get_database
from db.utils.rag_utils import retrieve_context_for_query

def display_politician_bio(name, politician_id):
    """Display basic biographical information about a politician."""
    print(f"\n===== {name}'s Biography =====")
    bio_db = get_database('biography')
    bio = bio_db.get_complete_biography(politician_id)
    
    # Basic info
    print(f"Name: {bio['politician']['name']}")
    if bio['politician']['birth_date']:
        print(f"Birth Date: {bio['politician']['birth_date']}")
    if bio['politician']['birth_place']:
        print(f"Birth Place: {bio['politician']['birth_place']}")
    
    # Education
    if bio['education']:
        print("\nEducation:")
        for edu in bio['education']:
            degree_info = f"{edu['degree']} in {edu['field_of_study']}" if edu['degree'] and edu['field_of_study'] else edu['degree'] or "Attended"
            years = f"{edu['start_year']} - {edu['end_year'] or 'present'}" if edu['start_year'] else ""
            print(f"- {edu['institution']}: {degree_info} ({years})")
    
    # Career
    if bio['career']:
        print("\nCareer:")
        for career in bio['career']:
            years = f"{career['start_year']} - {career['end_year'] or 'present'}" if career['start_year'] else ""
            print(f"- {career['role']} at {career['organization']} ({years})")
            if career['description']:
                print(f"  {career['description']}")
    
    # Family
    if bio['family']:
        print("\nFamily:")
        for member in bio['family']:
            print(f"- {member['relationship'].capitalize()}: {member['name']}")
            if member['description']:
                print(f"  {member['description']}")

def test_search_query(query, politician):
    """Test searching the databases with a query."""
    print(f"\n===== Query: '{query}' (for {politician}) =====")
    context = retrieve_context_for_query(query, politician)
    print(context)

def main():
    """Main function to run tests."""
    print("Political RAG Database System Test\n")
    
    # Display Trump and Biden bios
    display_politician_bio("Donald Trump", 1)
    display_politician_bio("Joe Biden", 2)
    
    # Test queries
    queries = [
        ("What is Donald Trump's education?", "Donald Trump"),
        ("What positions did Biden hold in government?", "Joe Biden"),
        ("When was Trump born?", "Donald Trump"),
        ("Tell me about Biden's family", "Joe Biden"),
    ]
    
    for query, politician in queries:
        test_search_query(query, politician)

if __name__ == "__main__":
    main()

================
File: test/test_trump_model.py
================
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig

def main():
    print("Loading Trump model from HuggingFace...")
    LORA_PATH = "nnat03/trump-mistral-adapter"
    BASE_MODEL_PATH = "mistralai/Mistral-7B-Instruct-v0.2"
    
    print(f"Using base model: {BASE_MODEL_PATH}")
    print(f"Using LoRA adapter: {LORA_PATH}")
    
    # Load base model
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16"
    )
    
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        print("Base model loaded successfully!")
        
        # Load LoRA model
        model = PeftModel.from_pretrained(base_model, LORA_PATH)
        print("LoRA adapter loaded successfully!")
        
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)
        print("Tokenizer loaded successfully!")
        
        print("All components loaded successfully!")
        return True
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        return False

if __name__ == "__main__":
    main()

================
File: training/train_mistral_biden.py
================
#!/usr/bin/env python3
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig
from transformers import DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import torch
from typing import Dict, Sequence, List
import pandas as pd
from zipfile import ZipFile

# Enable Memory-Efficient Loading with 4-bit Quantization
compute_dtype = torch.bfloat16  # Match Trump training
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

def clean_tweet(text: str) -> str:
    """Clean tweet text by removing URLs, handling mentions and hashtags"""
    import re
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # Convert @mentions to "someone"
    text = re.sub(r'@\w+', 'someone', text)
    # Remove hashtag symbol but keep the text
    text = re.sub(r'#(\w+)', r'\1', text)
    # Remove multiple spaces and trim
    return ' '.join(text.split()).strip()

def process_tweet(text: str) -> str:
    """Format tweet into instruction format"""
    text = clean_tweet(text)
    if not text:
        return ""
    return f"<s>[INST] Continue speaking in Joe Biden's style: [/INST] {text}</s>"

def process_speech(text: str) -> str:
    """Format speech into instruction format"""
    if not text or not isinstance(text, str):
        return ""
    text = text.strip()
    if not text:
        return ""
    return f"<s>[INST] Continue speaking in Joe Biden's style: [/INST] {text}</s>"

def load_tweets_dataset(zip_path: str) -> Dataset:
    """Load and process tweets from ZIP file"""
    print("Loading tweets dataset...")
    with ZipFile(zip_path) as zf:
        with zf.open('JoeBidenTweets.csv') as f:
            df = pd.read_csv(f)
    
    # Process tweets
    texts = []
    for tweet in df['tweet']:  # Using 'tweet' column
        processed = process_tweet(tweet)
        if processed:
            texts.append(processed)
    
    return Dataset.from_dict({"text": texts})

def load_speech_dataset(zip_path: str) -> Dataset:
    """Load and process speech from ZIP file"""
    print("Loading speech dataset...")
    with ZipFile(zip_path) as zf:
        with zf.open('joe_biden_dnc_2020.csv') as f:
            df = pd.read_csv(f)
    
    # Process speech segments
    texts = []
    for text in df['TEXT']:  # Using 'TEXT' column
        processed = process_speech(text)
        if processed:
            texts.append(processed)
    
    return Dataset.from_dict({"text": texts})

# Load base model and tokenizer
print("Loading base model and tokenizer...")
model_id = "mistralai/Mistral-7B-Instruct-v0.2"
print("Loading tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side="right", use_fast=False)
    print("Tokenizer loaded successfully")
except Exception as e:
    print(f"Error loading tokenizer: {str(e)}")
    raise
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=compute_dtype,
)

# Prepare model for training
print("Preparing model for training...")
model.config.use_cache = False
model.config.pretraining_tp = 1
model = prepare_model_for_kbit_training(model)

# Apply LoRA for fine-tuning
peft_config = LoraConfig(
    r=16,  # Match Trump training
    lora_alpha=64,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    inference_mode=False,
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Set padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load datasets from ZIP files
tweets_dataset = load_tweets_dataset("/home/natalie/datasets/biden/joe-biden-tweets.zip")
speech_dataset = load_speech_dataset("/home/natalie/datasets/biden/joe-biden-2020-dnc-speech.zip")

# Merge datasets
merged_dataset = Dataset.from_dict({
    "text": tweets_dataset["text"] + speech_dataset["text"]
})

# Split dataset
dataset = merged_dataset.train_test_split(test_size=0.1)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# Tokenization function
def tokenize_function(examples: Dict[str, Sequence[str]]) -> dict:
    """Tokenize with proper labels"""
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=1024,
        padding=False,
        return_tensors=None,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

# Process datasets
print("Tokenizing datasets...")
tokenized_train = train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing training dataset"
)
tokenized_eval = eval_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=eval_dataset.column_names,
    desc="Tokenizing validation dataset"
)

# Configure data collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    pad_to_multiple_of=8,
    return_tensors="pt",
    padding=True
)

# Training arguments - match Trump training exactly
training_args = TrainingArguments(
    output_dir="./mistral-biden",
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    save_steps=100,
    logging_steps=25,
    learning_rate=2e-4,
    num_train_epochs=3,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",
    bf16=True,
    optim="paged_adamw_8bit",
    logging_dir="./logs",
    report_to="none",
    gradient_checkpointing=True,
    save_total_limit=3,
    push_to_hub=False,
    ddp_find_unused_parameters=False,
    remove_unused_columns=False
)

# Initialize trainer
print("Initializing trainer...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=data_collator,
)

# Train
print("Starting training...")
try:
    model.train()
    trainer.train()
except Exception as e:
    print(f"An error occurred during training: {str(e)}")
    raise

# Save the model
print("Saving model...")
save_path = "./fine_tuned_biden_mistral"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("Training completed successfully!")

================
File: training/train_mistral_trump.py
================
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig
from transformers import DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset, concatenate_datasets
import torch
from typing import Dict, Sequence

# Step 1: Login to Hugging Face
HUGGING_FACE_KEY = "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"  # Replace with your Hugging Face token
login(token=HUGGING_FACE_KEY)

# Step 2: Enable Memory-Efficient Loading with 4-bit Quantization
compute_dtype = torch.bfloat16
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

# Step 3: Load the Mistral model and tokenizer
model_id = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side="right")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=compute_dtype,
)

# Prepare the model for training
model.config.use_cache = False
model.config.pretraining_tp = 1
model = prepare_model_for_kbit_training(model)

# Apply LoRA for fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=64,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    inference_mode=False,
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()  # Print trainable parameters info

# Set padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Step 4: Load the datasets
dataset1 = load_dataset("pookie3000/trump-interviews")  # Contains 'conversations'
dataset2 = load_dataset("bananabot/TrumpSpeeches")  # Might contain 'train'

# Step 5: Preprocess dataset1 (Trump Interviews)
def process_interview(example):
    """Format the conversation in Mistral's instruction format"""
    conversations = example["conversations"]
    
    # Safety check for conversation length
    if len(conversations) < 2:
        return {"text": ""}
        
    # Find first user and first assistant message
    user_msg = None
    assistant_msg = None
    
    for conv in conversations:
        if conv["role"].lower() == "user" and user_msg is None:
            user_msg = conv["content"]
        elif conv["role"].lower() == "assistant" and assistant_msg is None:
            assistant_msg = conv["content"]
            
        if user_msg and assistant_msg:
            break
    
    # Only format if we have both messages
    if user_msg and assistant_msg:
        return {"text": f"<s>[INST] {user_msg} [/INST] {assistant_msg}</s>"}
    return {"text": ""}

# Process interviews dataset and filter out empty examples
dataset1 = dataset1.map(
    process_interview,
    remove_columns=["conversations"],
    desc="Processing interviews"
)
dataset1 = dataset1.filter(lambda x: len(x["text"]) > 0, desc="Filtering valid interviews")

# Step 6: Process Trump Speeches dataset
def process_speech(example):
    """Format the speech in Mistral's instruction format"""
    text = example.get("text", "").strip()
    if not text:
        return {"text": ""}
    return {
        "text": f"<s>[INST] Continue speaking in Donald Trump's style: [/INST] {text}</s>"
    }

# Process speeches dataset and filter out empty examples
dataset2 = dataset2.map(
    process_speech,
    desc="Processing speeches"
)
dataset2 = dataset2.filter(lambda x: len(x["text"]) > 0, desc="Filtering valid speeches")

# Step 7: Merge the two datasets properly
merged_dataset = concatenate_datasets([dataset1["train"], dataset2["train"]])

# Step 8: Split dataset into 90% training and 10% evaluation
dataset = merged_dataset.train_test_split(test_size=0.1)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# Step 9: Tokenize the datasets
def tokenize_function(examples: Dict[str, Sequence[str]]) -> dict:
    """Tokenize with proper labels for casual language modeling"""
    # Tokenize the texts
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=1024,
        padding=False,
        return_tensors=None,
    )
    
    # Create labels for casual language modeling
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

# Process datasets with proper batching
tokenized_train_dataset = train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing training dataset"
)
tokenized_eval_dataset = eval_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=eval_dataset.column_names,
    desc="Tokenizing validation dataset"
)

# Configure data collator for sequence-to-sequence training
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    pad_to_multiple_of=8,
    return_tensors="pt",
    padding=True
)

# Step 10: Define optimized training arguments for 24GB VRAM
training_args = TrainingArguments(
    output_dir="./mistral-trump",
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    per_device_train_batch_size=4,  # Increased for 24GB VRAM
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    save_steps=100,
    logging_steps=25,
    learning_rate=2e-4,
    num_train_epochs=3,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",  # Changed to cosine for better convergence
    bf16=True,  # Changed to bf16 for better training stability
    optim="paged_adamw_8bit",  # Changed to 8-bit optimizer
    logging_dir="./logs",
    report_to="none",
    gradient_checkpointing=True,
    save_total_limit=3,
    push_to_hub=False,
    ddp_find_unused_parameters=False,
    remove_unused_columns=False
)

# Step 11: Set up training with proper error handling
try:
    print("Starting training...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_dataset,
        eval_dataset=tokenized_eval_dataset,
        data_collator=data_collator,
    )
    
    # Ensure model is in training mode
    model.train()
    
    # Start training
    trainer.train()
    
except Exception as e:
    print(f"An error occurred during training: {str(e)}")
    raise

# Step 12: Save the fine-tuned model
model.save_pretrained("./fine_tuned_trump_mistral")
tokenizer.save_pretrained("./fine_tuned_trump_mistral")

print("Training completed and model saved.")

================
File: .env.example
================
# HuggingFace API key with access to:
# - mistralai/Mistral-7B-Instruct-v0.2
# - Trump dataset
HUGGINGFACE_API_KEY=your_key_here

# Path where models will be saved/loaded
# Default: /home/shared_models/aipolitician
SHARED_MODELS_PATH=/path/to/models

================
File: .gitignore
================
# Environment files
.env
.env.*
!.env.example

# Python
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Specific __pycache__ directories to ignore
db/__pycache__/
db/schemas/__pycache__/
db/scripts/__pycache__/
db/utils/__pycache__/

# Virtual Environment
venv/
ENV/
env/
.env/
.venv/

# IDE
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Logs and databases
*.log
logs/
*.sqlite
*.db
*.db-journal
*.db-shm
*.db-wal
*.sqlite3

# Data exports
*.csv
*.xlsx

# Test coverage
.coverage
htmlcov/

# Training outputs
mistral-trump/
fine_tuned_trump_mistral/
training/mistral-trump/
training/fine_tuned_trump_mistral/

# Model checkpoints
checkpoints/
*.pt
*.pth
*.bin
*.model
*.ckpt

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb

# Local development
local_settings.py

# Additional __pycache__ directories to ignore
db/milvus/__pycache__/
db/milvus/scripts/__pycache__/
db/milvus/test/__pycache__/

# Milvus logs
db/milvus/logs/

================
File: chat_biden.py
================
#!/usr/bin/env python3
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch
from dotenv import load_dotenv
import os
import argparse
from pathlib import Path

# Add project root to the Python path
root_dir = Path(__file__).parent.absolute()
import sys
sys.path.insert(0, str(root_dir))

# Import database utils if they exist
try:
    from db.utils.rag_utils import integrate_with_chat
    HAS_RAG = True
except ImportError:
    HAS_RAG = False
    print("RAG database system not available. Running without RAG.")

# Load environment variables
load_dotenv()

def generate_response(prompt, model, tokenizer, use_rag=False, max_length=512):
    """Generate a response using the model, optionally with RAG"""
    # Use RAG if available and enabled
    if HAS_RAG and use_rag:
        # Get contextual information from the database
        context = integrate_with_chat(prompt, "Joe Biden")
        
        # Combine context with prompt
        rag_prompt = f"{context}\n\nUser Question: {prompt}"
        formatted_prompt = f"<s>[INST] {rag_prompt} [/INST]"
    else:
        # Standard prompt without RAG
        formatted_prompt = f"<s>[INST] {prompt} [/INST]"
    
    # Generate response
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            use_cache=True
        )
    
    # Decode and clean response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("[/INST]")[-1].strip()
    return response

def main():
    # Set up command line arguments
    parser = argparse.ArgumentParser(description="Chat with Biden AI model")
    parser.add_argument("--rag", action="store_true", help="Enable RAG for factual context")
    parser.add_argument("--max-length", type=int, default=512, help="Maximum response length")
    args = parser.parse_args()
    
    # Get model path from environment
    LORA_PATH = "nnat03/biden-mistral-adapter"
    
    # Load base model and tokenizer
    print("Loading model...")
    base_model_id = "mistralai/Mistral-7B-Instruct-v0.2"
    
    # Create BitsAndBytesConfig for 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,  # Match compute dtype with model dtype
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    # Load model with proper configuration
    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        attn_implementation="eager"  # Don't use Flash Attention
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=False)
    
    # Set padding token if needed
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load the fine-tuned LoRA weights
    print("Loading fine-tuned weights...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()  # Set to evaluation mode
    
    # Print status
    if HAS_RAG and args.rag:
        print("\nRAG system enabled. Using database for factual answers.")
    
    print("\nModel loaded! Enter your prompts (type 'quit' to exit)")
    print("\nExample prompts:")
    print("1. What's your vision for America's future?")
    print("2. How would you help the middle class?")
    print("3. Tell me about your infrastructure plan.")
    print("4. When were you born?")
    print("5. What was your position on the American Recovery and Reinvestment Act?")
    
    while True:
        try:
            prompt = input("\nYou: ")
            if prompt.lower() == 'quit':
                break
            
            # Generate response with or without RAG
            response = generate_response(
                prompt, 
                model, 
                tokenizer, 
                use_rag=(HAS_RAG and args.rag),
                max_length=args.max_length
            )
            print(f"\nBiden: {response}")
            
        except KeyboardInterrupt:
            print("\nExiting...")
            break
        except Exception as e:
            print(f"Error: {str(e)}")
            import traceback
            traceback.print_exc()  # Print full error trace for debugging
    
    # Cleanup
    print("\nCleaning up...")
    del model
    torch.cuda.empty_cache()

if __name__ == "__main__":
    main()

================
File: chat_trump.py
================
#!/usr/bin/env python3
"""
Chat with a Donald Trump AI model.

This module provides functionality to interact with a LLaMA model fine-tuned
on Donald Trump's speaking style and positions.
"""

import argparse  # Added for command-line arguments
import os
import sys
import gc
import torch
from pathlib import Path  # Added for path handling
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from dotenv import load_dotenv

# Add the root directory to the Python path for module resolution
root_dir = Path(__file__).resolve().parent
sys.path.append(str(root_dir))

# Add RAG utilities import with try/except
try:
    from db.utils.rag_utils import integrate_with_chat
    HAS_RAG = True
except ImportError:
    HAS_RAG = False
    print("RAG database system not available. Running without RAG.")

# Load environment variables
load_dotenv()

def generate_response(model, tokenizer, prompt, max_length=512, use_rag=False):
    """
    Generate a response using the model
    
    Args:
        model: The language model
        tokenizer: The tokenizer for the model
        prompt: The user's input prompt
        max_length: Maximum length of the generated response
        use_rag: Whether to use RAG for enhancing responses with facts
        
    Returns:
        The generated response text
    """
    # Use RAG to get context if available and enabled
    if HAS_RAG and use_rag:
        context = integrate_with_chat(prompt, "Donald Trump")
        rag_prompt = f"{context}\n\nUser Question: {prompt}"
        formatted_prompt = f"<s>[INST] {rag_prompt} [/INST]"
    else:
        formatted_prompt = f"<s>[INST] {prompt} [/INST]"
    
    # Generate response
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            use_cache=True
        )
    
    # Decode and clean response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("[/INST]")[-1].strip()
    return response

def main():
    # Add command-line arguments
    parser = argparse.ArgumentParser(description="Chat with Trump AI model")
    parser.add_argument("--rag", action="store_true", 
                    help="Enable RAG for factual context")
    parser.add_argument("--max-length", type=int, default=512, 
                    help="Maximum response length")
    args = parser.parse_args()
    
    # Get model path from environment
    LORA_PATH = "nnat03/trump-mistral-adapter"
    
    # Load base model and tokenizer
    print("Loading model...")
    base_model_id = "mistralai/Mistral-7B-Instruct-v0.2"
    
    # Create BitsAndBytesConfig for 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,  # Match compute dtype with model dtype
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    # Load model with proper configuration
    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        attn_implementation="eager"  # Don't use Flash Attention
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=False)
    
    # Set padding token if needed
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load the fine-tuned LoRA weights
    print("Loading fine-tuned weights...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()  # Set to evaluation mode
    
    # Show RAG status message
    if HAS_RAG and args.rag:
        print("\nRAG system enabled. Using database for factual answers.")
    
    print("\n🇺🇸 Trump AI Chat 🇺🇸")
    print("===================")
    print("Type 'quit', 'exit', or press Ctrl+C to end the conversation.")
    print("\nExample questions:")
    # Updated example prompts
    print("1. What's your plan for border security?")
    print("2. How would you handle trade with China?")
    print("3. Tell me about your tax reform achievements")
    print("4. When were you born?")
    print("5. What was your position on the Paris Climate Agreement?")
    
    while True:
        try:
            user_input = input("\nYou: ")
            if user_input.lower() in ['quit', 'exit']:
                break
                
            print("\nTrump: ", end="", flush=True)
            # Pass use_rag parameter to generate_response
            response = generate_response(model, tokenizer, user_input, 
                                      max_length=args.max_length,
                                      use_rag=args.rag)
            print(response)
            
        except KeyboardInterrupt:
            print("\nExiting chat...")
            break
        except Exception as e:
            print(f"Error: {str(e)}")
            import traceback
            traceback.print_exc()  # Print full error trace for debugging
    
    # Cleanup
    print("\nCleaning up...")
    del model
    del tokenizer
    torch.cuda.empty_cache()
    gc.collect()

if __name__ == "__main__":
    main()

================
File: README.md
================
# AI Politician

Fine-tuned Mistral-7B models that emulate Donald Trump's and Joe Biden's speaking styles.

## Prerequisites

### CUDA Setup
- CUDA compatible GPU required
- CUDA toolkit and drivers must be installed
- Note: The project was tested with CUDA 12.4

### Environment Setup
This project requires two separate conda environments due to specific version requirements:

1. Training Environment:
```bash
conda create -n training-env python=3.10
conda activate training-env
pip install -r requirements-training.txt
```

2. Chat Environment:
```bash
conda create -n chat-env python=3.10
conda activate chat-env
pip install -r requirements-chat.txt
```

### HuggingFace Setup
1. Create a HuggingFace account and get your API key
2. The API key needs access to:
   - mistralai/Mistral-7B-Instruct-v0.2
   - Trump dataset (specific access requirements)

### Environment Variables
1. Create a `.env` file from the example:
```bash
cp .env.example .env
```

2. Configure your environment:
```
HUGGINGFACE_API_KEY=your_key_here
SHARED_MODELS_PATH=/path/to/models  # Default: /home/shared_models/aipolitician
```

## Dataset Setup

### Required Datasets
1. Biden Datasets:
   - Joe Biden tweets dataset
   - Joe Biden 2020 DNC speech
   Place in: `/home/natalie/datasets/biden/`
   - joe-biden-tweets.zip
   - joe-biden-2020-dnc-speech.zip

2. Trump Datasets:
   - Trump interviews dataset
   - Trump speeches dataset
   (Specific dataset locations and formats to be documented)

## Training

### Important Note
Use the `python` command for training scripts in the training environment:

```bash
conda activate training-env
python training/train_mistral_trump.py
python training/train_mistral_biden.py
```

The training process:
- Loads Mistral-7B-Instruct-v0.2 base model
- Fine-tunes using LoRA
- Uses 4-bit quantization for memory efficiency
- Saves models to:
  - `mistral-trump/` and `fine_tuned_trump_mistral/`
  - `mistral-biden/` and `fine_tuned_biden_mistral/`

## Chat Interface

### Important Note
Use the `python3` command for chat scripts in the chat environment:

```bash
conda activate chat-env
python3 chat_trump.py  # Chat with Trump AI
python3 chat_biden.py  # Chat with Biden AI
```

Each script provides:
- Interactive terminal interface
- Type messages and get responses in the respective style
- Type 'quit' to exit
- Use Ctrl+C to exit at any time

## Database RAG System

The project includes a Retrieval-Augmented Generation (RAG) database system that provides factual information to the AI models. This system improves factual accuracy by retrieving relevant information from a set of structured databases.

### Database Setup

1. Create the database directory:
```bash
mkdir -p /home/natalie/Databases/political_rag
```

2. Initialize the databases:
```bash
conda activate chat-env
python -m db.scripts.initialize_databases
```

### Using RAG in Chat

By default, the chat interfaces will use the RAG system if available. To disable RAG:

```bash
python3 chat_biden.py --no-rag
python3 chat_trump.py --no-rag
```

### Database Structure

The system includes 17 specialized databases:
- Biography Database
- Policy Database
- Voting Record Database
- Public Statements Database
- And many more...

For full details, see the [Database README](db/README.md).

## Project Structure

```
.
├── .env                      # Environment variables
├── .env.example              # Example environment file
├── requirements-training.txt # Training environment dependencies
├── requirements-chat.txt     # Chat environment dependencies
├── chat_biden.py             # Biden chat interface
├── chat_trump.py             # Trump chat interface
├── db/                       # Database RAG system
│   ├── config.py             # Database configuration
│   ├── database.py           # Base database interface
│   ├── README.md             # Database documentation
│   ├── schemas/              # Database schema definitions
│   ├── scripts/              # Database scripts
│   └── utils/                # Database utilities
└── training/                 # Training code
    ├── train_mistral_biden.py
    └── train_mistral_trump.py
```

## Version Compatibility

### Training Environment
- Specific versions required for training stability
- See requirements-training.txt for exact versions
- Key packages:
  - torch==2.0.1
  - transformers==4.36.0
  - peft==0.7.0

### Chat Environment
- More flexible version requirements
- See requirements-chat.txt for minimum versions
- Key packages:
  - torch>=2.1.0
  - transformers>=4.36.0
  - peft>=0.7.0

## Troubleshooting

### Common Issues
1. Version Conflicts
   - Ensure you're using the correct conda environment
   - Check that you're using the right python command (python for training, python3 for chat)
   - Verify package versions match requirements files

2. CUDA Issues
   - Verify CUDA is properly installed
   - Check GPU availability with `nvidia-smi`
   - Ensure CUDA version compatibility

3. Model Loading Issues
   - Verify model paths in .env file
   - Check HuggingFace API key permissions
   - Ensure all required model files are present

For additional help or to report issues, please open a GitHub issue.

================
File: requirements-chat.txt
================
transformers>=4.36.0
peft>=0.7.0
torch>=2.1.0
accelerate>=0.25.0
bitsandbytes>=0.41.0
datasets>=2.15.0
python-dotenv>=1.0.0
safetensors>=0.4.0
huggingface-hub>=0.19.0
sentencepiece
protobuf

# Database RAG System Dependencies
requests>=2.28.0
beautifulsoup4>=4.11.0
sentence-transformers>=2.2.0
scikit-learn>=1.0.0
numpy>=1.22.0

================
File: requirements-training.txt
================
torch==2.0.1
transformers==4.36.0
tokenizers==0.15.2
accelerate==0.25.0
bitsandbytes==0.41.1
peft==0.7.0
datasets==2.15.0
numpy==1.24.3
pandas
python-dotenv
scipy==1.10.1
safetensors>=0.4.0
huggingface-hub>=0.19.0
triton==2.0.0
sentencepiece
protobuf



================================================================
End of Codebase
================================================================
